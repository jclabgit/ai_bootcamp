{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5ccc939",
      "metadata": {
        "id": "d5ccc939"
      },
      "source": [
        "\n",
        "# **Module 4: Generative AI Agents**\n",
        "\n",
        "Generative AI agents can automate tasks, generate content, enhance user experiences, and provide intelligent solutions. You can use platforms like Google Colab or Jupyter Notebook to practice programming with LLM models and explore their capabilities.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f61851c",
      "metadata": {
        "id": "6f61851c"
      },
      "source": [
        "\n",
        "## **4.1 Automating Tasks and Generating Content**\n",
        "\n",
        "### **Explanation**\n",
        "Generative AI agents can automate repetitive tasks and generate content, such as text, images, or even code. This can be used for applications like automated report generation, content creation, and more.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Task Description / Prompt ]  -->  [ LLM Processing: Content Generation ]  -->  [ Generated Content Output ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30570ce",
      "metadata": {
        "id": "e30570ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "content_generator = pipeline('text-generation', model='gpt-2')\n",
        "\n",
        "# Task: Generate content\n",
        "prompt = \"Generate a report summary about the importance of renewable energy.\"\n",
        "\n",
        "# Generate content\n",
        "generated_content = content_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Output\n",
        "print(generated_content[0]['generated_text'])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57873942",
      "metadata": {
        "id": "57873942"
      },
      "source": [
        "\n",
        "## **4.2 Prompt Engineering**\n",
        "\n",
        "### **Explanation**\n",
        "Prompt engineering involves crafting a specific prompt to generate the desired output from the LLM. This is the simplest form of interacting with an LLM, where the input directly influences the generated response.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Crafted Prompt ]  -->  [ LLM Processing: Prompt Engineering ]  -->  [ Generated Response ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c57fe98f",
      "metadata": {
        "id": "c57fe98f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "prompt_engineer = pipeline('text-generation', model='gpt-2')\n",
        "\n",
        "# Carefully crafted prompt\n",
        "prompt = \"Explain how solar energy can help reduce global warming.\"\n",
        "\n",
        "# Generate response\n",
        "response = prompt_engineer(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Output\n",
        "print(response[0]['generated_text'])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24d43255",
      "metadata": {
        "id": "24d43255"
      },
      "source": [
        "\n",
        "## **4.3 Short-term Memory Applications**\n",
        "\n",
        "### **Explanation**\n",
        "Short-term memory in generative AI involves maintaining context within a single session. This is particularly useful in conversational agents like chatbots that need to remember previous interactions within the same conversation.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input 1 ] --> [ LLM Processing: Short-term Memory ] --> [ Response 1 ]\n",
        "[ Input 2 ] --> [ LLM Processing: Short-term Memory ] --> [ Response 2 (Context-Aware) ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "117a774c",
      "metadata": {
        "id": "117a774c"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline\n",
        "chatbot = pipeline('text-generation', model=\"gpt2\")\n",
        "\n",
        "# Start the conversation\n",
        "conversation_history = \"Hello, how can solar energy help the environment?\"\n",
        "\n",
        "# Generate the first response\n",
        "response = chatbot(conversation_history, max_new_tokens=50, num_return_sequences=1)\n",
        "print(response[0]['generated_text'])\n",
        "\n",
        "# Continue the conversation, maintaining context\n",
        "conversation_history += \" \" + response[0]['generated_text']\n",
        "conversation_history += \" And how does it compare to wind energy?\"\n",
        "\n",
        "response = chatbot(conversation_history, max_new_tokens=50, num_return_sequences=1)\n",
        "print(response[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ad8f45",
      "metadata": {
        "id": "71ad8f45"
      },
      "source": [
        "\n",
        "## **4.4 External Data Integration**\n",
        "\n",
        "### **Explanation**\n",
        "External data integration allows the LLM to access and use additional data sources, such as APIs, to generate more accurate and contextually relevant responses. This is useful in applications like Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input Text ]  -->  [ LLM + External Data Source/API ]  -->  [ Generated Response with Contextual Data ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59735bdb",
      "metadata": {
        "id": "59735bdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "\n",
        "# External data source (API example)\n",
        "api_url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
        "response = requests.get(api_url)\n",
        "exchange_data = response.json()\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "data_integrator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Prompt with external data\n",
        "prompt = f\"The current exchange rate from USD to EUR is {exchange_data['rates']['EUR']}. Can you explain how exchange rates work?\"\n",
        "\n",
        "# Generate response\n",
        "generated_response = data_integrator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Output\n",
        "print(generated_response[0]['generated_text'])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75dc872",
      "metadata": {
        "id": "a75dc872"
      },
      "source": [
        "\n",
        "## **4.5 Tools Integration**\n",
        "\n",
        "### **Explanation**\n",
        "Tools integration involves combining the LLM with other tools or services to enhance its capabilities, such as using data analysis tools or custom APIs to perform specific tasks like text classification or customer feedback analysis.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input Text ]  -->  [ LLM + Integrated Tool ]  -->  [ Enhanced Output (e.g., Classification, Analysis) ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55d5d2a",
      "metadata": {
        "id": "a55d5d2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text classification pipeline\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "# Input text for classification\n",
        "text = \"The customer service was excellent and I am very satisfied with the product.\"\n",
        "\n",
        "# Perform classification (e.g., sentiment analysis)\n",
        "result = classifier(text)\n",
        "\n",
        "# Output\n",
        "print(result)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3317a1b4",
      "metadata": {
        "id": "3317a1b4"
      },
      "source": [
        "\n",
        "## **4.6 External Tools Interface**\n",
        "\n",
        "### **Explanation**\n",
        "This involves using the LLM as an interface to control and operate external tools, such as executing code, running scripts, or controlling IoT devices. This is useful for applications in automation and device management.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Command Input ]  -->  [ LLM Processing: Code Generation ]  -->  [ Execute Code / Control Device ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aac6910",
      "metadata": {
        "id": "2aac6910"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline, ideally with a model fine-tuned for code generation\n",
        "code_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Command to generate Python code\n",
        "prompt = \"Generate a Python script to print 'Hello, World!'\"\n",
        "\n",
        "# Generate code\n",
        "generated_code = code_generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "\n",
        "# Extract the generated text\n",
        "generated_text = generated_code[0]['generated_text']\n",
        "\n",
        "# Print the generated code\n",
        "print(\"Generated Code:\\n\")\n",
        "print(generated_text)\n",
        "\n",
        "# Validate and correct the generated code (simple correction for missing parentheses in 'print')\n",
        "if \"print '\" in generated_text or \"print '\" in generated_text:\n",
        "    generated_text = generated_text.replace(\"print '\", \"print('\").replace(\"'\", \"')\")\n",
        "\n",
        "# Safely execute the generated code (only if you are sure it's safe!)\n",
        "try:\n",
        "    exec(generated_text)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while executing the generated code: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f508aa96",
      "metadata": {
        "id": "f508aa96"
      },
      "source": [
        "\n",
        "## **4.7 Agent Frameworks**\n",
        "\n",
        "### **Explanation**\n",
        "Agent frameworks involve developing systems that manage multiple AI agents, enabling them to work together to perform complex tasks. This is useful in scenarios requiring coordination and collaboration among different AI systems.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Task ]  -->  [ LLM + Agent Framework ]  -->  [ Coordinated Multi-Agent Response ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebceed3",
      "metadata": {
        "id": "aebceed3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Placeholder for agent management framework\n",
        "\n",
        "# Example: Creating a simple agent framework\n",
        "class AgentFramework:\n",
        "    def __init__(self):\n",
        "        self.agents = []\n",
        "\n",
        "    def add_agent(self, agent):\n",
        "        self.agents.append(agent)\n",
        "\n",
        "    def perform_task(self, task):\n",
        "        for agent in self.agents:\n",
        "            agent.perform(task)\n",
        "\n",
        "class TextAgent:\n",
        "    def perform(self, task):\n",
        "        print(f\"TextAgent performing task: {task}\")\n",
        "\n",
        "class AnalysisAgent:\n",
        "    def perform(self, task):\n",
        "        print(f\"AnalysisAgent analyzing task: {task}\")\n",
        "\n",
        "# Initialize the framework and agents\n",
        "framework = AgentFramework()\n",
        "framework.add_agent(TextAgent())\n",
        "framework.add_agent(AnalysisAgent())\n",
        "\n",
        "# Perform a task using the framework\n",
        "framework.perform_task(\"Analyze customer feedback\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be1a348",
      "metadata": {
        "id": "9be1a348"
      },
      "source": [
        "\n",
        "## **4.8 Value Creation with Generative AI**\n",
        "\n",
        "### **Explanation**\n",
        "The conclusion focuses on understanding how generative AI can be used to create value by automating processes, generating products, and solving real-world challenges through the use of LLMs.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Real-World Problem ]  -->  [ LLM-Powered AI System ]  -->  [ Value Creation (Solutions, Automation) ]\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff3fa8a",
      "metadata": {
        "id": "1ff3fa8a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example of value creation through automation\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "automator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Problem: Need to generate daily summary reports automatically\n",
        "prompt = \"Generate a daily summary report on company sales performance.\"\n",
        "\n",
        "# Generate report\n",
        "daily_report = automator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Output\n",
        "print(\"Daily Summary Report:\")\n",
        "print(daily_report[0]['generated_text'])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.9 Fine-Tuning**\n",
        "\n",
        "### **Explanation**\n",
        "Fine-tuning is the process of taking a pre-trained LLM and further training it on a specific dataset to specialize it for a particular task or domain.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Pre-trained Model ] + [ Specialized Dataset ]  -->  [ Fine-tuning Process ]  -->  [ Fine-tuned Model ]\n",
        "```\n",
        "\n",
        "### **Python Implementation**"
      ],
      "metadata": {
        "id": "tW6M6vxxgSFn"
      },
      "id": "tW6M6vxxgSFn"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load your dataset (Example dataset for training)\n",
        "# Assuming dataset in the form of a list of (text, label) pairs\n",
        "train_texts = [\"I love programming\", \"I hate bugs\"]\n",
        "train_labels = [1, 0]  # 1: positive, 0: negative\n",
        "\n",
        "# Tokenize the dataset\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "\n",
        "# Prepare the dataset for training\n",
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "\n",
        "# Set up the trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./fine-tuned-model\")"
      ],
      "metadata": {
        "id": "IwHdmY5BgZbp"
      },
      "id": "IwHdmY5BgZbp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.10 Classification**\n",
        "\n",
        "### **Explanation**\n",
        "Classification involves assigning a label or category to an input based on its content. For instance, you can classify text as positive, negative, or neutral in sentiment analysis.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input Text ]  -->  [ LLM Processing: Classification ]  -->  [ Label/Category Output ]\n",
        "```\n",
        "\n",
        "### **Python Implementation**"
      ],
      "metadata": {
        "id": "2QfQJM1hgbpC"
      },
      "id": "2QfQJM1hgbpC"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text classification pipeline\n",
        "classifier = pipeline('text-classification')\n",
        "\n",
        "# Input text\n",
        "text = \"I love programming with Python!\"\n",
        "\n",
        "# Perform classification\n",
        "result = classifier(text)\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IPuiBq1Kgkok"
      },
      "id": "IPuiBq1Kgkok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.11 Regression**\n",
        "\n",
        "### **Explanation**\n",
        "Regression is a technique used to predict a continuous value (e.g., price, temperature) based on input features. LLMs can be adapted by framing the problem as text-based regression or using an LLM to extract features for a downstream regression model.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input Text ]  -->  [ LLM Feature Extraction ]  -->  [ Regression Model ]  -->  [ Continuous Value Output ]\n",
        "```\n",
        "\n",
        "### **Python Implementation**"
      ],
      "metadata": {
        "id": "K0TcH8wqg_rV"
      },
      "id": "K0TcH8wqg_rV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize BERT tokenizer and model for feature extraction\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input text\n",
        "texts = [\"I think the stock price will increase tomorrow.\",\n",
        "         \"The weather is likely to be warm today.\"]\n",
        "\n",
        "# Tokenize and extract features\n",
        "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Use the mean of the hidden states as features\n",
        "features = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Dummy target values (e.g., stock price change, temperature)\n",
        "targets = [1.5, 22.0]  # Replace with your actual target values\n",
        "\n",
        "# Train a regression model\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(features, targets)\n",
        "\n",
        "# Predict a value for a new text\n",
        "new_text = \"It might rain tomorrow.\"\n",
        "new_inputs = tokenizer(new_text, return_tensors='pt', padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    new_outputs = model(**new_inputs)\n",
        "new_features = new_outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Perform regression prediction\n",
        "predicted_value = regression_model.predict(new_features)\n",
        "\n",
        "# Output\n",
        "print(predicted_value)  # Continuous value prediction"
      ],
      "metadata": {
        "id": "PJFz9jOFhCEX"
      },
      "id": "PJFz9jOFhCEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.12 Clustering**\n",
        "\n",
        "### **Explanation**\n",
        "Clustering involves grouping similar inputs together based on their features. LLMs can be used to generate embeddings (vector representations) of text, which can then be clustered using traditional clustering algorithms like K-Means.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Input Texts ]  -->  [ LLM Feature Extraction ]  -->  [ Clustering Algorithm ]  -->  [ Cluster Labels Output ]\n",
        "```\n",
        "\n",
        "### **Python Implementation**"
      ],
      "metadata": {
        "id": "2enrYF0fhLZo"
      },
      "id": "2enrYF0fhLZo"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKN1Qyhqfvov"
      },
      "id": "kKN1Qyhqfvov"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Initialize BERT tokenizer and model for feature extraction\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input texts\n",
        "texts = [\n",
        "    \"I love programming with Python.\",\n",
        "    \"Artificial Intelligence is the future.\",\n",
        "    \"The weather is sunny today.\",\n",
        "    \"Python is a great language for data science.\",\n",
        "    \"It might rain tomorrow.\",\n",
        "    \"Machine Learning is a key technology in AI.\"\n",
        "]\n",
        "\n",
        "# Tokenize and extract features\n",
        "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Use the mean of the hidden states as features\n",
        "features = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Perform K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2)  # Set the number of clusters\n",
        "kmeans.fit(features)\n",
        "\n",
        "# Output cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Print cluster labels for each text\n",
        "for text, label in zip(texts, labels):\n",
        "    print(f\"Text: '{text}' is in Cluster {label}\")"
      ],
      "metadata": {
        "id": "lopC67YJhP5_"
      },
      "id": "lopC67YJhP5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.13 Reinforcement Learning**\n",
        "\n",
        "### **Explanation**\n",
        "Reinforcement Learning (RL) involves training an agent to make decisions by rewarding desirable actions and penalizing undesirable ones. LLMs can be used in conjunction with RL frameworks to develop intelligent agents that learn optimal behaviors.\n",
        "\n",
        "### **Schema**\n",
        "```\n",
        "[ Agent's Action ]  -->  [ Environment Response + Reward/Penalty ]  -->  [ LLM-based Policy Update ]  -->  [ Optimized Action ]\n",
        "```\n",
        "\n",
        "### **Python Implementation**"
      ],
      "metadata": {
        "id": "eV7_eBmvhV5f"
      },
      "id": "eV7_eBmvhV5f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for a basic RL setup with LLM-based policy guidance\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Example environment: simple grid world\n",
        "class GridWorld:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.state = (0, 0)  # Start at the top-left corner\n",
        "        self.goal = (size-1, size-1)  # Goal is at the bottom-right corner\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action: 0=up, 1=down, 2=left, 3=right\n",
        "        x, y = self.state\n",
        "        if action == 0 and x > 0: x -= 1\n",
        "        if action == 1 and x < self.size-1: x += 1\n",
        "        if action == 2 and y > 0: y -= 1\n",
        "        if action == 3 and y < self.size-1: y += 1\n",
        "\n",
        "        self.state = (x, y)\n",
        "\n",
        "        # Reward function\n",
        "        if self.state == self.goal:\n",
        "            return self.state, 1, True  # Reward of 1 for reaching the goal\n",
        "        else:\n",
        "            return self.state, -0.1, False  # Small penalty for each step\n",
        "\n",
        "# Basic LLM-guided policy (random actions as placeholder)\n",
        "def llm_guided_policy(state):\n",
        "    return random.choice([0, 1, 2, 3])\n",
        "\n",
        "# RL loop\n",
        "env = GridWorld(size=5)\n",
        "state = env.reset()\n",
        "\n",
        "for _ in range(100):\n",
        "    action = llm_guided_policy(state)\n",
        "    next_state, reward, done = env.step(action)\n",
        "\n",
        "    # Policy update would go here (currently random actions)\n",
        "\n",
        "    print(f\"State: {state} -> Action: {action} -> Reward: {reward} -> Next State: {next_state}\")\n",
        "\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Note: This is a simplified example. A full RL implementation would involve more complex policies and learning algorithms."
      ],
      "metadata": {
        "id": "iQAq4kWrhakT"
      },
      "id": "iQAq4kWrhakT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**\n",
        "\n",
        "This guide provided examples for various types of generative AI applications using LLMs, including fine-tuning, classification, regression, clustering, and reinforcement learning. Each example is formatted for Google Colab and illustrates how LLMs can be integrated with tools, used to automate tasks, and applied to create value in real-world scenarios.\n",
        "\n",
        "As you progress, consider how these techniques can be adapted and extended for your specific use cases or projects."
      ],
      "metadata": {
        "id": "ONo66L5thnPC"
      },
      "id": "ONo66L5thnPC"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}